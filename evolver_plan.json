{
  "title": "Auto-update plan 2025-10-04 (7 changements)",
  "rationale": "Aucun issues.md trouvé. Plan basé sur une inspection superficielle du code.",
  "changes": [
    {
      "file": "backend\\app\\db.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for db.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom typing import Any, Iterable, Optional\n\nimport psycopg\n\n\ndef get_db_url() -> str:\n    url = os.getenv(\"DATABASE_URL\")\n    if not url:\n        raise RuntimeError(\"DATABASE_URL manquant\")\n    # Normalize common variants (e.g., SQLAlchemy style 'postgresql+psycopg://') to libpq URI\n    if url.startswith(\"postgresql+psycopg://\"):\n        url = \"postgresql://\" + url.split(\"postgresql+psycopg://\", 1)[1]\n    if url.startswith(\"postgres+psycopg://\"):\n        url = \"postgresql://\" + url.split(\"postgres+psycopg://\", 1)[1]\n    if url.startswith(\"postgres://\"):\n        # psycopg accepts postgresql://, normalize older postgres://\n        url = \"postgresql://\" + url.split(\"postgres://\", 1)[1]\n    return url\n\n\ndef connect() -> psycopg.Connection:  # type: ignore\n    return psycopg.connect(get_db_url())\n\n\ndef log_event(stage: str, message: str, level: str = \"info\", meta: Optional[dict[str, Any]] = None) -> None:\n        try:\n                with connect() as conn:\n                        with conn.cursor() as cur:\n                                cur.execute(\n                                        \"INSERT INTO live_events(stage, level, message, meta) VALUES(%s,%s,%s,%s);\",\n                                        (stage, level, message, psycopg.types.json.Json(meta or {})),\n                                )\n                        conn.commit()\n        except Exception:\n                # Best effort logging; ignore errors\n                pass\n\n\ndef init_db() -> None:\n    # Ensure extension and tables exist; avoid failing the whole init if index creation isn't supported\n    with connect() as conn:\n        with conn.cursor() as cur:\n            # Extension might require superuser; ignore failure\n            try:\n                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n            except Exception:\n                pass\n            # sources\n            cur.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS sources (\n                    id SERIAL PRIMARY KEY,\n                    url TEXT UNIQUE NOT NULL,\n                    kind TEXT NOT NULL DEFAULT 'html',\n                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n                );\n                \"\"\"\n            )\n            # documents\n            cur.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS documents (\n                    id BIGSERIAL PRIMARY KEY,\n                    url TEXT UNIQUE NOT NULL,\n                    title TEXT,\n                    content TEXT,\n                    published_at TIMESTAMPTZ,\n                    lang TEXT,\n                    hash TEXT UNIQUE,\n                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n                    embedding vector(3072),\n                    indexed BOOLEAN NOT NULL DEFAULT FALSE\n                );\n                \"\"\"\n            )\n            # settings\n            cur.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS settings (\n                    key TEXT PRIMARY KEY,\n                    value JSONB,\n                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n                );\n                \"\"\"\n            )\n            # ci_status\n            cur.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS ci_status (\n                    id SMALLINT PRIMARY KEY DEFAULT 1,\n                    overall DOUBLE PRECISION,\n                    exact DOUBLE PRECISION,\n                    groundedness DOUBLE PRECISION,\n                    freshness DOUBLE PRECISION,\n                    report_path TEXT,\n                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n                );\n                \"\"\"\n            )\n            # ci_history\n            cur.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS ci_history (\n                    id SERIAL PRIMARY KEY,\n                    ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n                    overall DOUBLE PRECISION,\n                    exact DOUBLE PRECISION,\n                    groundedness DOUBLE PRECISION,\n                    semantic_f1 DOUBLE PRECISION,\n                    freshness DOUBLE PRECISION,\n                    avg_freshness_days DOUBLE PRECISION,\n                    meta JSONB\n                );\n                \"\"\"\n            )\n            # live_events\n            cur.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS live_events (\n                    id SERIAL PRIMARY KEY,\n                    ts TIMESTAMP DEFAULT NOW(),\n                    stage TEXT,\n                    level TEXT,\n                    message TEXT,\n                    meta JSONB\n                );\n                \"\"\"\n            )\n        conn.commit()\n\n    # Optional vector index creation\n    import os as _os\n    index_method = (_os.getenv(\"VECTOR_INDEX_METHOD\") or \"auto\").lower()\n    embedding_dim = 3072\n    create_stmt: Optional[str] = None\n    if index_method == \"hnsw\":\n        create_stmt = \"CREATE INDEX documents_embedding_hnsw_idx ON documents USING hnsw (embedding vector_cosine_ops);\"\n    elif index_method in (\"ivfflat\", \"auto\") and embedding_dim <= 2000:\n        create_stmt = \"CREATE INDEX documents_embedding_ivfflat_idx ON documents USING ivfflat (embedding vector_cosine_ops) WITH (lists=100);\"\n    if create_stmt:\n        try:\n            with connect() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\n                        f\"\"\"\n                        DO $$\n                        BEGIN\n                            IF NOT EXISTS (\n                                SELECT 1 FROM pg_class c JOIN pg_namespace n ON n.oid=c.relnamespace\n                                WHERE c.relname = 'documents_embedding_ivfflat_idx' OR c.relname = 'documents_embedding_hnsw_idx'\n                            ) THEN\n                                {create_stmt}\n                            END IF;\n                        END$$;\n                        \"\"\"\n                    )\n                conn.commit()\n        except Exception:\n            pass\n",
      "hazard": "low"
    },
    {
      "file": "backend\\app\\evolve.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for evolve.py.\"\"\"\n\nfrom __future__ import annotations\n\nfrom .db import log_event\n\n\ndef seed_from_docs(limit: int = 100) -> int:\n    \"\"\"Placeholder evolve seeding: simply logs an event and returns 0.\n\n    Real implementation would update embeddings / evaluation context.\n    \"\"\"\n    try:\n        log_event(\"evolve\", \"seed_from_docs placeholder\", meta={\"limit\": limit})\n    except Exception:\n        pass\n    return 0\n\n\n__all__ = [\"seed_from_docs\"]\n",
      "hazard": "low"
    },
    {
      "file": "backend\\app\\main.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for main.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom fastapi import FastAPI, HTTPException, Request\nfrom pydantic import BaseModel\nfrom typing import Any, List\nimport requests\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom .db import init_db, connect, log_event\nfrom .indexer import index_unembedded\nfrom crawler.run import crawl_sources, discover_new_sources\nfrom core.search import search_answer\nfrom .routes.admin import router as admin_router\nfrom .routes.search import router as search_router\nfrom contextlib import asynccontextmanager\nfrom .startup import seed_sources_if_empty\nfrom .tasks import celery_app  # for AsyncResult\nfrom celery.result import AsyncResult\nfrom .tasks import task_run_once, task_discover_once\nfrom .evolve import seed_from_docs\nimport requests as _requests_diag\nfrom .db import connect\nfrom typing import Optional\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # type: ignore[name-defined]\n    # Initialize DB only if DATABASE_URL is provided; allows local dev without Docker/DB\n    try:\n        init_db()\n        seed_sources_if_empty()\n    except Exception:\n        pass\n    yield\n\n\napp = FastAPI(title=\"AI Auto-Evolve Backend\", lifespan=lifespan)\napp.add_middleware(\n    CORSMiddleware,\n    # Explicit origins to satisfy browsers when credentials are allowed\n    allow_origins=[\n        \"http://localhost:3000\",\n        \"http://127.0.0.1:3000\",\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\napp.include_router(admin_router)\napp.include_router(search_router)\n\n\n\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n\n\n@app.get(\"/sources\")\ndef list_sources(limit: int = 100, offset: int = 0):\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"SELECT id, url, kind, created_at FROM sources ORDER BY id ASC LIMIT %s OFFSET %s;\",\n                    (limit, offset),\n                )\n                rows = cur.fetchall()\n        return {\"items\": [{\"id\": r[0], \"url\": r[1], \"kind\": r[2], \"created_at\": r[3].isoformat()} for r in rows]}\n    except Exception:\n        return {\"items\": []}\n\n\n@app.get(\"/docs/latest\")\ndef docs_latest(limit: int = 10):\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT url, title, published_at, lang FROM documents ORDER BY created_at DESC LIMIT %s;\", (limit,))\n                rows = cur.fetchall()\n        return {\"items\": [{\"url\": r[0], \"title\": r[1], \"date\": r[2].isoformat() if r[2] else None, \"lang\": r[3]} for r in rows]}\n    except Exception:\n        return {\"items\": []}\n\n\n@app.get(\"/metrics\")\ndef metrics():\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT COUNT(*) FROM documents;\")\n                docs = cur.fetchone()[0]\n                cur.execute(\"SELECT COUNT(*) FROM sources;\")\n                sources = cur.fetchone()[0]\n                # Pull last CI status and threshold (from env or stored setting)\n                cur.execute(\"SELECT overall, exact, groundedness, freshness, updated_at FROM ci_status WHERE id=1;\")\n                ci = cur.fetchone()\n                cur.execute(\"SELECT value FROM settings WHERE key='DISCOVERY_QUERIES';\")\n                row = cur.fetchone()\n        ci_status = None\n        if ci:\n            ci_status = {\n                \"overall\": float(ci[0]) if ci[0] is not None else None,\n                \"exact\": float(ci[1]) if ci[1] is not None else None,\n                \"groundedness\": float(ci[2]) if ci[2] is not None else None,\n                \"freshness\": float(ci[3]) if ci[3] is not None else None,\n                \"updated_at\": ci[4].isoformat() + \"Z\" if ci[4] else None,\n            }\n        eval_threshold = os.getenv(\"EVAL_MIN_OVERALL_SCORE\", \"0.75\")\n        discover_qs = None\n        try:\n            if row and isinstance(row[0], dict):\n                discover_qs = row[0].get(\"queries\")\n        except Exception:\n            discover_qs = None\n        # Keep legacy fields and add UI-friendly fields expected by frontend\n        return {\n            \"nb_docs\": docs,\n            \"nb_sources\": sources,\n            \"last_update\": datetime.utcnow().isoformat() + \"Z\",\n            \"eval_score\": None,\n            # UI expected keys\n            \"documents\": docs,\n            \"coverage\": 1.0 if sources > 0 else 0.0,\n            \"freshness_days\": None,\n            \"avg_response_time\": None,\n            \"ci\": ci_status,\n            \"eval_threshold\": float(eval_threshold) if eval_threshold else None,\n            \"discovery_queries\": discover_qs,\n        }\n    except Exception:\n        return {\n            \"nb_docs\": 0,\n            \"nb_sources\": 0,\n            \"last_update\": datetime.utcnow().isoformat() + \"Z\",\n            \"eval_score\": None,\n            \"documents\": 0,\n            \"coverage\": 0.0,\n            \"freshness_days\": None,\n            \"avg_response_time\": None,\n        }\n\n\n@app.post(\"/ingest/crawl\")\ndef ingest_crawl(limit: int = 10):\n    try:\n        n = crawl_sources(limit=limit)\n        return {\"inserted\": n}\n    except Exception as e:\n        return {\"inserted\": 0, \"error\": str(e)}\n\n\n@app.post(\"/ingest/discover\")\ndef ingest_discover(per_query: int = 5, max_new: int = 25, queries: str | None = None):\n    try:\n        qs = None\n        if queries:\n            qs = [q.strip() for q in queries.split(\",\") if q.strip()]\n        n = discover_new_sources(queries=qs, per_query=per_query, max_new=max_new)\n        return {\"new_sources\": n}\n    except Exception as e:\n        return {\"new_sources\": 0, \"error\": str(e)}\n\n\n@app.post(\"/ingest/discover_async\")\ndef ingest_discover_async(per_query: int = 5, max_new: int = 25, queries: str | None = None):\n    try:\n        qs = None\n        if queries:\n            qs = [q.strip() for q in queries.split(\",\") if q.strip()]\n        async_res = task_discover_once.delay(per_query=per_query, max_new=max_new, queries=qs)\n        return {\"status\": \"ok\", \"task_id\": async_res.id}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\n@app.post(\"/ingest/index\")\ndef ingest_index(batch_size: int = 10):\n    try:\n        n = index_unembedded(batch_size=batch_size)\n        return {\"indexed\": n}\n    except Exception as e:\n        return {\"indexed\": 0, \"error\": str(e)}\n\n\n@app.get(\"/search\")\ndef search(q: str, k: int = 6):\n    try:\n        res = search_answer(q, top_k=k)\n        # Map to UI-expected shape (sources instead of citations) while preserving original fields\n        sources = []\n        for c in res.get(\"citations\", []):\n            title = c.get(\"title\") or \"\"\n            url = c.get(\"url\") or \"\"\n            sources.append([title, url])\n        out = dict(res)\n        out[\"query\"] = q\n        out[\"sources\"] = sources\n        return out\n    except Exception as e:\n        return {\"query\": q, \"answer\": \"Je ne sais pas\", \"citations\": [], \"sources\": [], \"confidence\": 0.0, \"error\": str(e)}\n\n\n# Minimal jobs endpoint(s) for UI compatibility and polling\n@app.get(\"/jobs\")\ndef list_jobs(status: str | None = None, type: str | None = None):\n    # We don't track history yet; return empty list with filters echoed\n    return {\"items\": [], \"status\": status, \"type\": type}\n\n\n@app.get(\"/jobs/{task_id}\")\ndef job_status(task_id: str):\n    try:\n        res: AsyncResult = AsyncResult(task_id, app=celery_app)\n        state = res.state\n        out: dict = {\"task_id\": task_id, \"state\": state}\n        if res.successful():\n            out[\"result\"] = res.result\n            out[\"status\"] = res.result.get(\"status\", \"ok\") if isinstance(res.result, dict) else \"ok\"\n        elif res.failed():\n            out[\"status\"] = \"error\"\n            out[\"error\"] = str(res.result)\n        return out\n    except Exception as e:\n        return {\"task_id\": task_id, \"state\": \"UNKNOWN\", \"status\": \"error\", \"error\": str(e)}\n\n\nclass SourceCreate(BaseModel):\n    url: str\n    type: str = \"html\"\n    allowed: bool | None = None\n\n\n@app.post(\"/sources\")\ndef create_source(body: SourceCreate):\n    kind = body.type or \"html\"\n    url = body.url\n    if not url:\n        raise HTTPException(status_code=400, detail=\"url manquante\")\n    with connect() as conn:\n        with conn.cursor() as cur:\n            cur.execute(\n                \"INSERT INTO sources(url, kind) VALUES(%s,%s) ON CONFLICT (url) DO NOTHING RETURNING id;\",\n                (url, kind),\n            )\n            row = cur.fetchone()\n        conn.commit()\n    if not row:\n        # Already exists, fetch id\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT id FROM sources WHERE url=%s;\", (url,))\n                row = cur.fetchone()\n    return {\"id\": row[0] if row else None}\n\n\n@app.delete(\"/sources/{source_id}\")\ndef delete_source_simple(source_id: int):\n    with connect() as conn:\n        with conn.cursor() as cur:\n            cur.execute(\"DELETE FROM sources WHERE id=%s;\", (source_id,))\n            deleted = cur.rowcount or 0\n        conn.commit()\n    return {\"status\": \"ok\", \"deleted\": deleted}\n\n\n# Additional endpoints for frontend/workflows compatibility\nclass IngestRunBody(BaseModel):\n    source_ids: list[int] | None = None\n    new_url: str | None = None\n\n\n@app.post(\"/ingest/run\")\ndef ingest_run(body: IngestRunBody | None = None):\n    created_source_id = None\n    try:\n        # Snapshot metrics before\n        def _counts():\n            try:\n                with connect() as conn:\n                    with conn.cursor() as cur:\n                        cur.execute(\"SELECT COUNT(*) FROM sources;\")\n                        s = cur.fetchone()[0]\n                        cur.execute(\"SELECT COUNT(*) FROM documents;\")\n                        d = cur.fetchone()[0]\n                return int(s), int(d)\n            except Exception:\n                return 0, 0\n\n        sources_before, docs_before = _counts()\n\n        # Ensure seed if empty\n        try:\n            seed_sources_if_empty()\n        except Exception:\n            pass\n\n        if body and body.new_url:\n            with connect() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\n                        \"INSERT INTO sources(url, kind) VALUES(%s,%s) ON CONFLICT (url) DO NOTHING RETURNING id;\",\n                        (body.new_url, \"html\"),\n                    )\n                    row = cur.fetchone()\n                conn.commit()\n            created_source_id = row[0] if row else None\n        # Actively discover new sources before crawling (tunable via env)\n        discovered = 0\n        try:\n            per_query = int(os.getenv(\"DISCOVERY_PER_QUERY\", \"5\"))\n            max_new = int(os.getenv(\"DISCOVERY_MAX_NEW\", \"25\"))\n            discovered = discover_new_sources(per_query=per_query, max_new=max_new)\n        except Exception:\n            discovered = 0\n        # Crawl more aggressively to reflect discovery\n        crawl_limit = int(os.getenv(\"CRAWLER_RUN_ONCE_LIMIT\", \"50\"))\n        index_batch = int(os.getenv(\"INDEX_RUN_ONCE_BATCH\", \"50\"))\n        ins = crawl_sources(limit=crawl_limit)\n        idx = index_unembedded(batch_size=index_batch)\n\n        sources_after, docs_after = _counts()\n        return {\n            \"status\": \"ok\",\n            \"inserted\": ins,\n            \"indexed\": idx,\n            \"created_source_id\": created_source_id,\n            \"discovered\": discovered,\n            \"new_sources_added\": max(0, sources_after - sources_before),\n            \"docs_before\": docs_before,\n            \"docs_after\": docs_after,\n            \"sources_before\": sources_before,\n            \"sources_after\": sources_after,\n            \"task_id\": None,\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e), \"inserted\": 0, \"indexed\": 0, \"task_id\": None}\n\n\n@app.post(\"/ingest/run_async\")\ndef ingest_run_async():\n    \"\"\"Trigger background discovery+crawl+index and return a task id immediately.\"\"\"\n    try:\n        per_query = int(os.getenv(\"DISCOVERY_PER_QUERY\", \"5\"))\n        max_new = int(os.getenv(\"DISCOVERY_MAX_NEW\", \"25\"))\n        crawl_limit = int(os.getenv(\"CRAWLER_RUN_ONCE_LIMIT\", \"50\"))\n        index_batch = int(os.getenv(\"INDEX_RUN_ONCE_BATCH\", \"50\"))\n        async_res = task_run_once.delay(per_query=per_query, max_new=max_new, crawl_limit=crawl_limit, index_batch=index_batch)\n        return {\"status\": \"ok\", \"task_id\": async_res.id}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\n@app.get(\"/events\")\ndef get_events(limit: int = 100):\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"SELECT ts, stage, level, message, meta FROM live_events ORDER BY id DESC LIMIT %s;\",\n                    (limit,),\n                )\n                rows = cur.fetchall()\n        return {\n            \"items\": [\n                {\"ts\": r[0].isoformat(), \"stage\": r[1], \"level\": r[2], \"message\": r[3], \"meta\": (r[4] or {})}\n                for r in rows\n            ]\n        }\n    except Exception as e:\n        return {\"items\": [], \"error\": str(e)}\n\n\nclass EventIn(BaseModel):\n    stage: str\n    level: str = \"info\"\n    message: str\n    meta: dict | None = None\n    token: str | None = None\n\n\n@app.post(\"/events\")\ndef post_event(body: EventIn):\n    # Simple token-based auth to allow CI to push events\n    expected = os.getenv(\"EVENTS_API_TOKEN\", \"\")\n    if not expected:\n        # If no token configured on server, reject to avoid abuse\n        raise HTTPException(status_code=403, detail=\"events disabled\")\n    if (body.token or \"\") != expected:\n        raise HTTPException(status_code=401, detail=\"invalid token\")\n    try:\n        log_event(body.stage, body.message, level=body.level, meta=body.meta or {})\n        # If payload contains evaluator scores, persist to ci_status for dashboard\n        try:\n            meta = body.meta or {}\n            if body.stage == \"evolve\" and isinstance(meta, dict) and (\"overall\" in meta or \"aggregates\" in meta):\n                overall = float(meta.get(\"overall\") or meta.get(\"score\") or 0)\n                exact = None\n                grounded = None\n                fresh = None\n                ag = meta.get(\"aggregates\") if isinstance(meta.get(\"aggregates\"), dict) else {}\n                if ag:\n                    exact = ag.get(\"exact\")\n                    grounded = ag.get(\"groundedness\")\n                    fresh = ag.get(\"freshness\")\n                with connect() as conn:\n                    with conn.cursor() as cur:\n                        cur.execute(\n                            \"INSERT INTO ci_status(id, overall, exact, groundedness, freshness, updated_at) VALUES(1,%s,%s,%s,%s,NOW()) \"\n                            \"ON CONFLICT (id) DO UPDATE SET overall=EXCLUDED.overall, exact=EXCLUDED.exact, groundedness=EXCLUDED.groundedness, freshness=EXCLUDED.freshness, updated_at=NOW();\",\n                            (overall, exact, grounded, fresh),\n                        )\n                    conn.commit()\n        except Exception:\n            pass\n        return {\"status\": \"ok\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/evolve/run\")\ndef evolve_run():\n    import os as _os\n    import requests as _requests\n    try:\n        # Trim to avoid hidden spaces/newlines causing 404 on workflow dispatch\n        token = (_os.getenv(\"GITHUB_TOKEN\") or _os.getenv(\"GH_TOKEN\") or \"\").strip()\n        repo = (_os.getenv(\"GITHUB_REPOSITORY\") or \"\").strip()\n        ref = (_os.getenv(\"GITHUB_REF\", \"main\") or \"main\").strip()\n        if not token or not repo:\n            msg = \"GITHUB_TOKEN et GITHUB_REPOSITORY requis dans l'environnement pour déclencher la CI.\"\n            log_event(\"evolve\", msg, level=\"warn\")\n            return {\"status\": \"error\", \"error\": msg}\n        # Optionally verify workflow exists before dispatch to give clearer 404 cause\n        try:\n            wfs = _requests.get(f\"https://api.github.com/repos/{repo}/actions/workflows\", headers={\n                \"Authorization\": f\"Bearer {token}\",\n                \"Accept\": \"application/vnd.github+json\",\n            }, timeout=10)\n            if wfs.status_code == 200:\n                names = [wf.get(\"path\") for wf in wfs.json().get(\"workflows\", [])]\n                if \".github/workflows/auto-evolve.yml\" not in names:\n                    log_event(\"evolve\", \"Workflow auto-evolve.yml introuvable (liste workflows)\", level=\"error\", meta={\"paths\": names})\n                    return {\"status\": \"error\", \"code\": 404, \"hint\": \"Workflow auto-evolve.yml absent dans la branche ref\", \"workflows\": names}\n        except Exception:\n            pass\n        url = f\"https://api.github.com/repos/{repo}/actions/workflows/auto-evolve.yml/dispatches\"\n        r = _requests.post(url, headers={\n            \"Authorization\": f\"Bearer {token}\",\n            \"Accept\": \"application/vnd.github+json\",\n        }, json={\"ref\": ref})\n        if r.status_code in (204, 201):\n            log_event(\"evolve\", \"Workflow auto-evolve déclenché\", meta={\"repo\": repo, \"ref\": ref})\n            return {\"status\": \"ok\"}\n        else:\n            # Fournir un diagnostic plus explicite pour les erreurs communes\n            diagnostic: dict[str, Any] = {\"status_code\": r.status_code, \"response\": r.text}\n            try:\n                diagnostic[\"rate_limit_remaining\"] = r.headers.get(\"x-ratelimit-remaining\")\n                diagnostic[\"rate_limit_reset\"] = r.headers.get(\"x-ratelimit-reset\")\n            except Exception:\n                pass\n            if r.status_code == 403:\n                hint = (\n                    \"403 GitHub API: La plupart du temps dû à un token sans le scope 'workflow'. \"\n                    \"Assure-toi que: 1) Le PAT est un token classic avec scopes: repo, workflow (ou un fine-grained avec Actions: Read+Write). \"\n                    \"2) Le repo ciblé correspond exactement à GITHUB_REPOSITORY='owner/repo'. \"\n                    \"3) Le workflow existe sous .github/workflows/auto-evolve.yml et contient 'workflow_dispatch:'. \"\n                    \"4) Si tu utilises le GITHUB_TOKEN GitHub Actions natif, l'appel externe depuis un poste local ne fonctionnera pas; utilise un PAT personnel.\"\n                )\n                diagnostic[\"hint\"] = hint\n            elif r.status_code == 404:\n                diagnostic[\"hint\"] = (\n                    \"404: Vérifie le nom du workflow (auto-evolve.yml) et la valeur de GITHUB_REPOSITORY.\" \n                    \" Le fichier doit être dans la branche indiquée par ref.\" )\n            log_event(\"evolve\", \"Échec du déclenchement CI\", level=\"error\", meta=diagnostic)\n            return {\"status\": \"error\", **diagnostic}\n    except Exception as e:\n        log_event(\"evolve\", f\"Erreur: {e}\", level=\"error\")\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\n@app.get(\"/evolve/workflows\")\ndef evolve_list_workflows():\n    import os as _os\n    token = (_os.getenv(\"GITHUB_TOKEN\") or _os.getenv(\"GH_TOKEN\") or \"\").strip()\n    repo = (_os.getenv(\"GITHUB_REPOSITORY\") or \"\").strip()\n    if not token or not repo:\n        return {\"status\": \"error\", \"error\": \"GITHUB_TOKEN ou GITHUB_REPOSITORY manquant\"}\n    url = f\"https://api.github.com/repos/{repo}/actions/workflows\"\n    try:\n        r = _requests_diag.get(url, headers={\n            \"Authorization\": f\"Bearer {token}\",\n            \"Accept\": \"application/vnd.github+json\",\n        }, timeout=15)\n        data = {}\n        try:\n            data = r.json()\n        except Exception:\n            data = {\"raw\": r.text}\n        workflows = [\n            {\n                \"name\": wf.get(\"name\"),\n                \"path\": wf.get(\"path\"),\n                \"id\": wf.get(\"id\"),\n                \"state\": wf.get(\"state\"),\n                \"created_at\": wf.get(\"created_at\"),\n                \"updated_at\": wf.get(\"updated_at\"),\n            }\n            for wf in data.get(\"workflows\", [])\n        ]\n        return {\"status\": \"ok\", \"http_status\": r.status_code, \"count\": len(workflows), \"workflows\": workflows}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\n@app.post(\"/metrics/record\")\ndef metrics_record(payload: dict):\n    \"\"\"Record a CI/evaluation run. Expected fields: overall, exact, groundedness, semantic_f1, freshness, avg_freshness_days, meta\"\"\"\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"INSERT INTO ci_history(overall, exact, groundedness, semantic_f1, freshness, avg_freshness_days, meta) VALUES(%s,%s,%s,%s,%s,%s,%s);\",\n                    (\n                        payload.get(\"overall\"),\n                        payload.get(\"exact\"),\n                        payload.get(\"groundedness\"),\n                        payload.get(\"semantic_f1\"),\n                        payload.get(\"freshness\"),\n                        payload.get(\"avg_freshness_days\"),\n                        _requests_diag.types.json.Json(payload.get(\"meta\") or {}),\n                    ),\n                )\n            conn.commit()\n        return {\"status\": \"ok\"}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\n@app.get(\"/metrics/history\")\ndef metrics_history(limit: Optional[int] = 50):\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT id, ts, overall, exact, groundedness, semantic_f1, freshness, avg_freshness_days, meta FROM ci_history ORDER BY ts DESC LIMIT %s;\", (limit,))\n                rows = cur.fetchall()\n        items = [\n            {\n                \"id\": r[0],\n                \"ts\": r[1].isoformat() if r[1] else None,\n                \"overall\": r[2],\n                \"exact\": r[3],\n                \"groundedness\": r[4],\n                \"semantic_f1\": r[5],\n                \"freshness\": r[6],\n                \"avg_freshness_days\": r[7],\n                \"meta\": r[8],\n            }\n            for r in rows\n        ]\n        return {\"items\": items}\n    except Exception as e:\n        return {\"items\": [], \"error\": str(e)}\n\n\n@app.post(\"/index/build\")\ndef index_build():\n    try:\n        n = index_unembedded(batch_size=25)\n        return {\"status\": \"ok\", \"task_id\": None, \"indexed\": n}\n    except Exception as e:\n        return {\"status\": \"error\", \"task_id\": None, \"indexed\": 0, \"error\": str(e)}\n\n\n@app.post(\"/evolve/seed_from_docs\")\ndef evolve_seed_from_docs(limit: int = 200, trigger_ci: bool = False):\n    try:\n        out = seed_from_docs(limit=limit)\n        # Optionally trigger CI evolve workflow so PR includes updated topics/issues\n        if trigger_ci:\n            try:\n                _ = evolve_run()  # reuse handler to trigger workflow and log events\n                out[\"workflow_triggered\"] = True\n            except Exception:\n                out[\"workflow_triggered\"] = False\n        return {\"status\": \"ok\", **out}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\nclass IndexActivateBody(BaseModel):\n    index_version_id: int\n    threshold_score: float | None = 0\n\n\n@app.post(\"/index/activate\")\ndef index_activate(body: IndexActivateBody):\n    # Placeholder: no versioning implemented yet\n    return {\"status\": \"ok\", \"index_version_id\": body.index_version_id}\n\n\n@app.get(\"/index/versions\")\ndef index_versions():\n    return {\"items\": []}\n\n\nclass EvaluateRunBody(BaseModel):\n    sets: list[str] | None = None\n\n\n@app.post(\"/evaluate/run\")\ndef evaluate_run(body: EvaluateRunBody | None = None):\n    # Placeholder: trigger local evaluator asynchronously in a real setup\n    return {\"status\": \"ok\", \"task_id\": None}\n\n\n@app.get(\"/evaluate/recent\")\ndef evaluate_recent(limit: int = 5):\n    return {\"items\": []}\n\n\n# Settings CRUD for admin (persist discovery queries and others)\nclass SettingIn(BaseModel):\n    key: str\n    value: dict | list | str | int | float | None\n\n\n@app.get(\"/admin/settings\")\ndef get_settings():\n    try:\n        out: dict[str, Any] = {}\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT key, value FROM settings;\")\n                for k, v in cur.fetchall():\n                    out[k] = v\n        # Provide env fallback preview for known toggles if not in DB\n        if \"EVENTS_VERBOSE\" not in out:\n            out[\"EVENTS_VERBOSE\"] = os.getenv(\"EVENTS_VERBOSE\", \"0\")\n        if \"CRAWLER_OBEY_ROBOTS\" not in out:\n            out[\"CRAWLER_OBEY_ROBOTS\"] = os.getenv(\"CRAWLER_OBEY_ROBOTS\", \"1\")\n        # Live search related fallbacks\n        if \"ENABLE_LIVE_SEARCH\" not in out:\n            out[\"ENABLE_LIVE_SEARCH\"] = os.getenv(\"ENABLE_LIVE_SEARCH\", \"0\")\n        if \"LIVE_SEARCH_MODE\" not in out:\n            out[\"LIVE_SEARCH_MODE\"] = os.getenv(\"LIVE_SEARCH_MODE\", \"low\")\n        if \"LIVE_SEARCH_MAX_RESULTS\" not in out:\n            out[\"LIVE_SEARCH_MAX_RESULTS\"] = os.getenv(\"LIVE_SEARCH_MAX_RESULTS\", \"3\")\n        return {\"items\": out}\n    except Exception as e:\n        return {\"items\": {}, \"error\": str(e)}\n\n\n@app.post(\"/admin/settings\")\ndef upsert_setting(body: SettingIn):\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"INSERT INTO settings(key, value, updated_at) VALUES(%s, %s, NOW()) \"\n                    \"ON CONFLICT (key) DO UPDATE SET value=EXCLUDED.value, updated_at=NOW();\",\n                    (body.key, body.value),\n                )\n            conn.commit()\n        return {\"status\": \"ok\"}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n\n\n# Evaluator: provide helper to get publish dates for URLs to compute freshness\n@app.post(\"/evaluator/publish_dates\")\ndef evaluator_publish_dates(urls: List[str]):\n    try:\n        results: dict[str, str | None] = {}\n        with connect() as conn:\n            with conn.cursor() as cur:\n                for u in urls:\n                    cur.execute(\"SELECT published_at FROM documents WHERE url=%s;\", (u,))\n                    row = cur.fetchone()\n                    results[u] = row[0].isoformat() if row and row[0] else None\n        return {\"items\": results}\n    except Exception as e:\n        return {\"items\": {}, \"error\": str(e)}\n\n\n@app.post(\"/sources/{source_id}/test\")\ndef source_test_connectivity(source_id: int):\n    url = None\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT url FROM sources WHERE id=%s;\", (source_id,))\n                row = cur.fetchone()\n        url = row[0] if row else None\n    except Exception:\n        url = None\n    if not url:\n        raise HTTPException(status_code=404, detail=\"source introuvable\")\n    try:\n        r = requests.get(url, timeout=10, headers={\"User-Agent\": \"connectivity-check/1.0\"})\n        return {\"ok\": bool(r.ok), \"status\": r.status_code}\n    except Exception as e:\n        return {\"ok\": False, \"message\": str(e)}\n",
      "hazard": "low"
    },
    {
      "file": "backend\\app\\startup.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for startup.py.\"\"\"\n\nfrom __future__ import annotations\n\nfrom .db import connect, log_event\n\nSEED_SOURCES = [\n    (\"https://openai.com/blog\", \"html\"),\n    (\"https://huggingface.co/blog\", \"html\"),\n    (\"https://stability.ai/blog\", \"html\"),\n]\n\n\ndef seed_sources_if_empty() -> int:\n    \"\"\"Insert a small set of default sources if table is empty.\"\"\"\n    try:\n        with connect() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"SELECT COUNT(*) FROM sources;\")\n                count = cur.fetchone()[0]\n                if count and count > 0:\n                    return 0\n                inserted = 0\n                for url, kind in SEED_SOURCES:\n                    try:\n                        cur.execute(\n                            \"INSERT INTO sources(url, kind) VALUES(%s,%s) ON CONFLICT (url) DO NOTHING;\",\n                            (url, kind),\n                        )\n                        inserted += cur.rowcount or 0\n                    except Exception:\n                        pass\n            conn.commit()\n        if inserted:\n            log_event(\"seed\", \"Seed sources inserted\", meta={\"count\": inserted})\n        return inserted\n    except Exception:\n        return 0\n",
      "hazard": "low"
    },
    {
      "file": "backend\\app\\tasks.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for tasks.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom celery import Celery\n\nfrom crawler.run import crawl_sources, discover_new_sources\nfrom .indexer import index_unembedded\nfrom .db import log_event, connect\nfrom .evolve import seed_from_docs\nimport subprocess\nimport json\nimport os as _os\nfrom datetime import datetime\n\n\ndef _get_env(name: str, default: str) -> str:\n    v = os.getenv(name)\n    return v if v is not None else default\n\n\nredis_url = _get_env(\"REDIS_URL\", \"redis://localhost:6379/0\")\n\ncelery_app = Celery(\"auto_evolve\", broker=redis_url, backend=redis_url)\n\n\n@celery_app.task\ndef task_crawl_once(limit: int = 10) -> int:\n    log_event(\"crawl\", \"Periodic crawl start\", meta={\"limit\": limit})\n    n = crawl_sources(limit=limit)\n    log_event(\"crawl\", \"Periodic crawl done\", meta={\"inserted\": n})\n    return n\n\n\n@celery_app.task\ndef task_index_once(batch_size: int = 10) -> int:\n    log_event(\"index\", \"Periodic index start\", meta={\"batch_size\": batch_size})\n    n = index_unembedded(batch_size=batch_size)\n    log_event(\"index\", \"Periodic index done\", meta={\"indexed\": n})\n    # Non-blocking seed of evolve context if enabled via env\n    try:\n        if os.getenv(\"EVOLVE_SEED_AFTER_INDEX\", \"1\") in (\"1\", \"true\", \"True\"):\n            seed_from_docs(limit=200)\n    except Exception:\n        pass\n    return n\n\n\n@celery_app.on_after_configure.connect\ndef setup_periodic_tasks(sender, **kwargs):  # type: ignore\n    # Every 5 minutes, discover, crawl and then index\n    sender.add_periodic_task(300.0, celery_app.signature(\"backend.app.tasks.task_discover_once\"), name=\"discover every 5m\")\n    sender.add_periodic_task(300.0, task_crawl_once.s(10), name=\"crawl every 5m\")\n    sender.add_periodic_task(300.0, task_index_once.s(10), name=\"index every 5m\")\n    # Daily evaluator run at 03:30 UTC (configurable via env)\n    try:\n        eval_schedule = float(_os.getenv(\"EVAL_PERIOD_SECONDS\", str(24 * 3600)))\n        sender.add_periodic_task(eval_schedule, celery_app.signature(\"backend.app.tasks.task_evaluate_and_record\"), name=\"evaluate daily\")\n    except Exception:\n        pass\n\n\n@celery_app.task(name=\"backend.app.tasks.task_discover_once\")\ndef task_discover_once(per_query: int = 5, max_new: int = 25, queries: list[str] | None = None) -> int:\n    # Load persisted DISCOVERY_QUERIES if not explicitly provided\n    if queries is None:\n        try:\n            with connect() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\"SELECT value FROM settings WHERE key='DISCOVERY_QUERIES';\")\n                    row = cur.fetchone()\n                    if row and isinstance(row[0], dict):\n                        arr = row[0].get(\"queries\")\n                        if isinstance(arr, list) and arr:\n                            queries = [str(x) for x in arr if str(x).strip()]\n        except Exception:\n            pass\n    log_event(\"discover\", \"Periodic discover start\", meta={\"per_query\": per_query, \"max_new\": max_new, \"has_queries\": bool(queries)})\n    n = discover_new_sources(queries=queries, per_query=per_query, max_new=max_new)\n    log_event(\"discover\", \"Periodic discover done\", meta={\"new_sources\": n})\n    return n\n\n\n@celery_app.task(name=\"backend.app.tasks.task_run_once\")\ndef task_run_once(per_query: int = 5, max_new: int = 25, crawl_limit: int = 50, index_batch: int = 50) -> dict:\n    \"\"\"Run discovery, then crawl, then index in the background and return a summary dict.\"\"\"\n    try:\n        discovered = 0\n        try:\n            discovered = discover_new_sources(per_query=per_query, max_new=max_new)\n        except Exception:\n            discovered = 0\n        log_event(\"discover\", \"Run once discover\", meta={\"new_sources\": discovered})\n        inserted = 0\n        indexed = 0\n        try:\n            inserted = crawl_sources(limit=crawl_limit)\n        except Exception:\n            inserted = 0\n        log_event(\"crawl\", \"Run once crawl\", meta={\"inserted\": inserted})\n        try:\n            indexed = index_unembedded(batch_size=index_batch)\n        except Exception:\n            indexed = 0\n        log_event(\"index\", \"Run once index\", meta={\"indexed\": indexed})\n        try:\n            if os.getenv(\"EVOLVE_SEED_AFTER_INDEX\", \"1\") in (\"1\", \"true\", \"True\"):\n                seed_from_docs(limit=200)\n        except Exception:\n            pass\n        return {\n            \"status\": \"ok\",\n            \"discovered\": int(discovered),\n            \"inserted\": int(inserted),\n            \"indexed\": int(indexed),\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e), \"discovered\": 0, \"inserted\": 0, \"indexed\": 0}\n\n\n@celery_app.task(name=\"backend.app.tasks.task_evaluate_and_record\")\ndef task_evaluate_and_record(version_id: int = 1, testset: str | None = None) -> dict:\n    \"\"\"Run evaluator CLI and record its report into ci_history. Uses evaluator/evaluate.py --version-id <id>\"\"\"\n    try:\n        log_event(\"evolve\", \"Evaluator run start\", meta={\"version_id\": version_id})\n        cmd = [\"python\", \"evaluator/evaluate.py\", \"--version-id\", str(version_id)]\n        if testset:\n            cmd += [\"--testset\", testset]\n        # Run evaluator and capture report path (it writes evaluator/reports/index_<id>.json)\n        subprocess.check_call(cmd)\n        report_path = Path(\"evaluator/reports\") / f\"index_{version_id}.json\"\n        if report_path.exists():\n            with report_path.open(\"r\", encoding=\"utf-8\") as f:\n                report = json.load(f)\n            agg = report.get(\"aggregates\", {})\n            overall = report.get(\"overall_score\") or report.get(\"overall_score\")\n            payload = {\n                \"overall\": float(report.get(\"overall_score\", 0.0)),\n                \"exact\": float(agg.get(\"exact\", 0.0)),\n                \"groundedness\": float(agg.get(\"groundedness\", 0.0)),\n                \"semantic_f1\": float(report.get(\"aggregates\", {}).get(\"semantic_f1\", 0.0)),\n                \"freshness\": float(agg.get(\"freshness\", 0.0)),\n                \"avg_freshness_days\": float(agg.get(\"avg_freshness_days\")) if agg.get(\"avg_freshness_days\") is not None else None,\n                \"meta\": {\"version_id\": version_id, \"ts\": datetime.utcnow().isoformat()},\n            }\n            # Try direct DB insert first\n            try:\n                from psycopg import connect as _pg_connect\n                with _pg_connect(_os.getenv(\"DATABASE_URL\")) as conn:\n                    with conn.cursor() as cur:\n                        cur.execute(\n                            \"INSERT INTO ci_history(overall, exact, groundedness, semantic_f1, freshness, avg_freshness_days, meta) VALUES(%s,%s,%s,%s,%s,%s,%s);\",\n                            (\n                                payload[\"overall\"],\n                                payload[\"exact\"],\n                                payload[\"groundedness\"],\n                                payload[\"semantic_f1\"],\n                                payload[\"freshness\"],\n                                payload[\"avg_freshness_days\"],\n                                json.dumps(payload[\"meta\"]),\n                            ),\n                        )\n                    conn.commit()\n            except Exception:\n                # Fallback to calling the REST endpoint\n                try:\n                    import requests\n                    requests.post((_os.getenv(\"EVAL_BACKEND_URL\") or _os.getenv(\"BACKEND_URL\") or \"http://localhost:8000\") + \"/metrics/record\", json=payload, timeout=10)\n                except Exception:\n                    pass\n            log_event(\"evolve\", \"Evaluator run recorded\", meta={\"version_id\": version_id})\n            return {\"status\": \"ok\", \"payload\": payload}\n        else:\n            log_event(\"evolve\", \"Evaluator report not found\", level=\"error\", meta={\"path\": str(report_path)})\n            return {\"status\": \"error\", \"error\": \"report not found\"}\n    except Exception as e:\n        log_event(\"evolve\", f\"Evaluator run failed: {e}\", level=\"error\")\n        return {\"status\": \"error\", \"error\": str(e)}\n",
      "hazard": "low"
    },
    {
      "file": "backend\\app\\routes\\admin.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for admin.py.\"\"\"\n\nfrom __future__ import annotations\n\nfrom fastapi import APIRouter\n\nrouter = APIRouter(prefix=\"/admin\", tags=[\"admin\"])\n\n\n@router.get(\"/ping\")\nasync def admin_ping():\n    return {\"status\": \"ok\", \"scope\": \"admin\"}\n",
      "hazard": "low"
    },
    {
      "file": "backend\\app\\routes\\search.py",
      "patch": "PATCH:WRITE\n\"\"\"Auto-added module docstring for search.py.\"\"\"\n\nfrom __future__ import annotations\n\nfrom fastapi import APIRouter\n\n# Clean minimal placeholder; real search endpoints can be added later.\nrouter = APIRouter(prefix=\"/search\", tags=[\"search\"])\n\n\n@router.get(\"/ping\")\nasync def search_ping():\n    return {\"status\": \"ok\", \"scope\": \"search\"}\n",
      "hazard": "low"
    }
  ],
  "tests": [
    {
      "file": "tests/test_smoke_backend.py",
      "content": "import requests\n\ndef test_health():\n    r = requests.get('http://localhost:8000/health', timeout=5)\n    assert r.ok and r.json().get('status') == 'ok'\n"
    }
  ]
}